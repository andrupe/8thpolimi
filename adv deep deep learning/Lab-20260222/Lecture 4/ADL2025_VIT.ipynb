{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Vision Transformers"
      ],
      "metadata": {
        "id": "OJvxv_aoqfqU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this practical session we will build a **Vision Transformer (ViT)** for image classification using **pytorch**.\n",
        "\n",
        "This approach was popularized by Dosovitskiy et al., \"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale\" ([link to paper](https://openreview.net/forum?id=YicbFdNTTy)), who were the first to successfully train a tranformer on ImageNet.\n",
        "\n",
        "We will build a smaller model to perform image classification on [CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html).\n",
        "\n",
        "This section is based on Eugenio Lomurno's TensorFlow ViT notebook from the [ANNDL class](https://boracchi.faculty.polimi.it/teaching/AN2DL.htm) and on a [tutorial](https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial15/Vision_Transformer.html) by Phillip Lippe.\n",
        "\n",
        "Let's start with some standard imports:"
      ],
      "metadata": {
        "id": "Sly-_jlQs-QB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bAUrseKDQpjL"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's set all random seeds for reproducibility:"
      ],
      "metadata": {
        "id": "wneaVHbmqppe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "seed = 42\n",
        "\n",
        "def seed_everything(seed):\n",
        "  random.seed(seed)\n",
        "  np.random.seed(seed)\n",
        "  torch.manual_seed(seed)\n",
        "  if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "seed_everything(seed)"
      ],
      "metadata": {
        "id": "yFhnEwR9Rgfe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some visualization settings:"
      ],
      "metadata": {
        "id": "_OfHcxS4q0lx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set(font_scale=1.4)\n",
        "sns.set_style('white')\n",
        "plt.rc('font', size=14)\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "Qv6mN52zSL-c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preprocessing"
      ],
      "metadata": {
        "id": "EPGTM2aCs5kk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalize training images and apply some basic data augmentation\n",
        "train_transform = transforms.Compose([\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandomResizedCrop((32,32),scale=(0.8,1.0),ratio=(0.9,1.1)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5,), (0.5,))\n",
        "    ])\n",
        "\n",
        "# Test (and validation) images are just normalized\n",
        "test_transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5,), (0.5,))\n",
        "    ])\n",
        "\n",
        "# Download training data applying the preprocessing defined above\n",
        "# Data are split into training and validation sets\n",
        "# A seeding trick is used to make sure data augmentation is not applied to\n",
        "# validation data\n",
        "trainset = datasets.CIFAR10(root='./data', train=True, download=True,\n",
        "                            transform=train_transform)\n",
        "\n",
        "valset = datasets.CIFAR10(root='./data', train=True, download=True,\n",
        "                              transform=test_transform)\n",
        "seed_everything(42)\n",
        "trainset, _ = torch.utils.data.random_split(trainset, [45000, 5000])\n",
        "seed_everything(42)\n",
        "_, valset = torch.utils.data.random_split(valset, [45000, 5000])\n",
        "\n",
        "# We will access training and validation data through data loaders\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128,\n",
        "                                          shuffle=True)\n",
        "\n",
        "valloader = torch.utils.data.DataLoader(valset, batch_size=128,\n",
        "                                        shuffle=False)\n",
        "\n",
        "# Download test data\n",
        "testset = datasets.CIFAR10(root='./data', train=False, download=True,\n",
        "                           transform=test_transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=128, shuffle=False)\n",
        "\n",
        "# These are the names of the classes\n",
        "# Later, we will use these for more than visualization...\n",
        "class_names = ('airplane', 'automobile', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
      ],
      "metadata": {
        "id": "v3GbKf7ZRpiF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see some images from the dataset to get an idea of the task.\n",
        "\n",
        "The images are 32x32 RGB and there are 10 classes."
      ],
      "metadata": {
        "id": "mzciGH3ay_4R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to display the image with pyplot (denormalize and transpose)\n",
        "def to_plt(img):\n",
        "  return np.transpose(img, (1, 2, 0)) * 0.5 + 0.5\n",
        "\n",
        "\n",
        "# Function to visualise a subset of the dataset\n",
        "def visualize_dataset(dataset, class_names):\n",
        "    # Set up a 5x5 grid for images\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    for i in range(25):\n",
        "        # Select a random image index\n",
        "        idx = np.random.randint(0, len(dataset))\n",
        "        # Add subplot for the current image\n",
        "        plt.subplot(5, 5, i + 1)\n",
        "        # Remove axis markers\n",
        "        plt.xticks([])\n",
        "        plt.yticks([])\n",
        "        plt.grid(False)\n",
        "        # Display the image\n",
        "        x = dataset[idx][0]\n",
        "        plt.imshow(to_plt(x))\n",
        "        # Set the title using the class name\n",
        "        plt.title(class_names[dataset[idx][1]])\n",
        "    # Show the visualisation\n",
        "    plt.show()\n",
        "\n",
        "# Visualise a selection of training images\n",
        "visualize_dataset(trainset, class_names)"
      ],
      "metadata": {
        "id": "iDqmPtp2YPtD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Differently from Convolutional Neural Networks, we will not apply receptive fields to the images. Instead, we will process each image a a **sequence** of...patches.\n",
        "\n",
        "Each 32x32 image will be represented as a sequence of 64 4x4 patches:"
      ],
      "metadata": {
        "id": "_FRGHNHezpCq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Input image dimensions\n",
        "input_shape = (32, 32, 3)\n",
        "\n",
        "# Size of each patch (4x4)\n",
        "patch_size = 4\n",
        "\n",
        "# Total number of patches\n",
        "num_patches = (input_shape[0] // patch_size) ** 2\n",
        "\n",
        "# Function to turn an image into a sequence of patches\n",
        "def img_to_patch(x, patch_size, flatten_channels=True):\n",
        "    \"\"\"\n",
        "    Inputs:\n",
        "        x - torch.Tensor representing the image of shape [B, C, H, W]\n",
        "        patch_size - Number of pixels per dimension of the patches (integer)\n",
        "        flatten_channels - If True, the patches will be returned in a flattened format\n",
        "                           as a feature vector instead of a image grid.\n",
        "    \"\"\"\n",
        "    B, C, H, W = x.shape\n",
        "    x = x.reshape(B, C, H//patch_size, patch_size, W//patch_size, patch_size)\n",
        "    x = x.permute(0, 2, 4, 1, 3, 5) # [B, H', W', C, p_H, p_W]\n",
        "    x = x.flatten(1,2)              # [B, H'*W', C, p_H, p_W]\n",
        "    if flatten_channels:\n",
        "        x = x.flatten(2,4)          # [B, H'*W', C*p_H*p_W]\n",
        "    return x"
      ],
      "metadata": {
        "id": "Nsjh2Wst0Bq_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's try this on an image from our training set"
      ],
      "metadata": {
        "id": "iwD-uq9y0kuo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pick an image\n",
        "sample_img = trainset[1][0]\n",
        "plt.axis('off')\n",
        "plt.imshow(to_plt(sample_img))\n",
        "\n",
        "# Decompose it into patches\n",
        "img_patches = img_to_patch(sample_img[None], patch_size=4,\n",
        "                           flatten_channels=False)[0]\n",
        "\n",
        "def visualize_patches(patches, grid=False):\n",
        "    # Extract patches from the input image\n",
        "    n_patches = patches.shape[0]\n",
        "\n",
        "    # Determine the grid size for visualising patches\n",
        "    side = int(np.sqrt(n_patches))\n",
        "    plt.figure(figsize=(6, 6))\n",
        "\n",
        "    # Plot each patch\n",
        "    for i in range(n_patches):\n",
        "      if grid:\n",
        "        plt.subplot(side, side, i + 1)\n",
        "      else:\n",
        "        plt.subplot(n_patches, 1, i + 1)\n",
        "      patch = to_plt(patches[i])\n",
        "      plt.imshow(patch)\n",
        "      plt.axis('off')\n",
        "    if grid:\n",
        "      plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Visualise the patches\n",
        "visualize_patches(img_patches, grid=True)\n",
        "visualize_patches(img_patches, grid=False)"
      ],
      "metadata": {
        "id": "030UCXbXWxw9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This was for visualization, but the 3x4x4 patches will be actually processed as **flat** vectors of size 48:"
      ],
      "metadata": {
        "id": "lrYjZ66g1zMY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "flat_img_patches = img_to_patch(sample_img[None], patch_size=4,\n",
        "                           flatten_channels=True)[0]\n",
        "\n",
        "print(\"Shape of original image: \", sample_img.shape)\n",
        "print(\"Shape of sequence of patches: \", img_patches.shape)\n",
        "print(\"Shape of sequence of FLATTENED patches: \", flat_img_patches.shape)"
      ],
      "metadata": {
        "id": "vI74RsKOei3S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Definition"
      ],
      "metadata": {
        "id": "jOQLlI5d340z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our model will be a stack of Pre-Layer Normalization (Pre-LN) Multi-Head Attention blocks:\n",
        "\n",
        "![image.png](https://uvadlc-notebooks.readthedocs.io/en/latest/_images/pre_layer_norm.svg)\n",
        "\n",
        "Let's start by defining our self-attention block with Pre-LN.\n",
        "\n",
        "**TODO:** complete the definition of the attention block by implementing the `forward` method.\n",
        "The following layers are already defined in the `__init__` using `torch.nn`:\n",
        "\n",
        "\n",
        "*   Two [Layer Norm](https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html) operations\n",
        "*   A [Multi-Head Attention](https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html) layer to be used for **self-attention**\n",
        "*   A simple feed-forward network\n",
        "\n",
        "Refer to the image above (b) and to pytorch documentation. Don't forget about the residual connections!"
      ],
      "metadata": {
        "id": "ZL92ojAn2tsB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pre-LN Attention block\n",
        "class AttentionBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, embed_dim, hidden_dim, num_heads, dropout=0.0):\n",
        "        \"\"\"\n",
        "        Inputs:\n",
        "            embed_dim - Dimensionality of input and attention feature vectors\n",
        "            hidden_dim - Dimensionality of hidden layer in feed-forward network\n",
        "                         (usually 2-4x larger than embed_dim)\n",
        "            num_heads - Number of heads to use in the Multi-Head Attention block\n",
        "            dropout - Amount of dropout to apply in the feed-forward network\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        # Layer Norm\n",
        "        self.layer_norm_1 = nn.LayerNorm(embed_dim)\n",
        "\n",
        "        # Multi-Head Attention\n",
        "        # takes query, key, value\n",
        "        # returns outputs and attention weights\n",
        "        self.attn = nn.MultiheadAttention(embed_dim, num_heads,\n",
        "                                          dropout=dropout)\n",
        "\n",
        "        # Layer Norm\n",
        "        self.layer_norm_2 = nn.LayerNorm(embed_dim)\n",
        "\n",
        "        # FFN (Feed Forward Network)\n",
        "        self.linear = nn.Sequential(\n",
        "            nn.Linear(embed_dim, hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, embed_dim),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = ... # YOUR CODE HERE\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "TSmQhu2fnkuX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can build our ViT model by stacking:\n",
        "\n",
        "\n",
        "*   A **linear** embedding of **flat** patches of dimension 3x4x4 = 48 into dimension `embed_dim` (256)\n",
        "*   To this input embedding we concatenate a **learnable positional embedding** and a **class token**\n",
        "*   A sequence of `num_layers` (6) of the attention blocks defined above\n",
        "*   An MLP classification head *applied to the class token*\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "fgPkueEyRc3H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class VisionTransformer(nn.Module):\n",
        "\n",
        "    def __init__(self, embed_dim=256, hidden_dim=512, num_channels=3,\n",
        "                 num_heads=8, num_layers=6, num_classes=10, patch_size=4,\n",
        "                 num_patches=64, dropout=0.2):\n",
        "        \"\"\"\n",
        "        Inputs:\n",
        "            embed_dim - Dimensionality of the input feature vectors to the Transformer\n",
        "            hidden_dim - Dimensionality of the hidden layer in the feed-forward networks\n",
        "                         within the Transformer\n",
        "            num_channels - Number of channels of the input (3 for RGB)\n",
        "            num_heads - Number of heads to use in the Multi-Head Attention block\n",
        "            num_layers - Number of layers to use in the Transformer\n",
        "            num_classes - Number of classes to predict\n",
        "            patch_size - Number of pixels that the patches have per dimension\n",
        "            num_patches - Maximum number of patches an image can have\n",
        "            dropout - Amount of dropout to apply in the feed-forward network and\n",
        "                      on the input encoding\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.patch_size = patch_size\n",
        "\n",
        "        # Layers/Networks\n",
        "        self.input_layer = nn.Linear(num_channels*(patch_size**2), embed_dim)\n",
        "        self.transformer = nn.Sequential(*[AttentionBlock(embed_dim, hidden_dim,\n",
        "                        num_heads, dropout=dropout) for _ in range(num_layers)])\n",
        "        self.mlp_head = nn.Sequential(\n",
        "            nn.LayerNorm(embed_dim),\n",
        "            nn.Linear(embed_dim, num_classes)\n",
        "        )\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # Parameters/Embeddings\n",
        "        self.cls_token = nn.Parameter(torch.randn(1,1,embed_dim))\n",
        "        self.pos_embedding = nn.Parameter(torch.randn(1,1+num_patches,embed_dim))\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Preprocess input image into patches\n",
        "        x = img_to_patch(x, self.patch_size)\n",
        "        B, T, _ = x.shape\n",
        "        x = self.input_layer(x)\n",
        "\n",
        "        # Add CLS token and positional encoding\n",
        "        cls_token = self.cls_token.repeat(B, 1, 1)\n",
        "        x = torch.cat([cls_token, x], dim=1)\n",
        "        x = x + self.pos_embedding[:,:T+1]\n",
        "\n",
        "        # Apply Transforrmer\n",
        "        x = self.dropout(x)\n",
        "        x = x.transpose(0, 1)\n",
        "        x = self.transformer(x)\n",
        "\n",
        "        # Perform classification prediction\n",
        "        cls = x[0]\n",
        "        out = self.mlp_head(cls)\n",
        "        return out"
      ],
      "metadata": {
        "id": "QQYBmxZrqO9z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "K59If9InUZpB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Try to use GPU:"
      ],
      "metadata": {
        "id": "TYsNibX1UL6H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print (device)"
      ],
      "metadata": {
        "id": "QoH0SwmDrN9m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We use cross entropy loss, Adam optimizer, and use our validation set to perform early stopping"
      ],
      "metadata": {
        "id": "2dR_v9lVUV73"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# **Initialize Model, Loss, Optimizer**\n",
        "model = VisionTransformer().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# **Early Stopping Parameters**\n",
        "patience = 5  # Stop training if no improvement in 'patience' epochs\n",
        "best_val_loss = float(\"inf\")\n",
        "early_stop_counter = 0\n",
        "\n",
        "# **Prepare the training**\n",
        "epochs = 10\n",
        "model.train()"
      ],
      "metadata": {
        "id": "NaHUmXJSript"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training loop (you may want to skip this on colab)"
      ],
      "metadata": {
        "id": "GqxxJvzJUpmW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# (DON'T) TRY THIS AT HOME\n",
        "%%script false --no-raise-error\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    total_loss = 0\n",
        "    model.train()\n",
        "\n",
        "    for images, labels in trainloader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        # Convert labels to match known class indices\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_train_loss = total_loss / len(trainloader)\n",
        "\n",
        "    # **Validation Phase**\n",
        "    model.eval()\n",
        "    total_val_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in valloader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            total_val_loss += loss.item()\n",
        "\n",
        "    avg_val_loss = total_val_loss / len(valloader)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "    # **Early Stopping Check**\n",
        "    if avg_val_loss < best_val_loss:\n",
        "        best_val_loss = avg_val_loss\n",
        "        early_stop_counter = 0  # Reset counter since we improved\n",
        "        # Save the best model\n",
        "        os.makedirs(\"output_vit_cifar10\", exist_ok=True)\n",
        "        model_path = \"output_vit_cifar10/best_model.pth\"\n",
        "        torch.save(model.state_dict(), model_path)\n",
        "        print(f\"New best model saved with Val Loss: {avg_val_loss:.4f}\")\n",
        "    else:\n",
        "        early_stop_counter += 1\n",
        "        print(f\"No improvement for {early_stop_counter}/{patience} epochs.\")\n",
        "\n",
        "    # Stop training if no improvement for `patience` epochs\n",
        "    if early_stop_counter >= patience:\n",
        "        print(f\"Early stopping triggered after {epoch+1} epochs.\")\n",
        "        break\n",
        "\n",
        "print(f\"Training completed. Best model saved at: {model_path}\")"
      ],
      "metadata": {
        "id": "yCF8xdMGsnu_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing\n",
        "\n",
        "Let's measure the accuracy of our model.\n",
        "\n",
        "Load trained weights:"
      ],
      "metadata": {
        "id": "vYW8jwSGVURy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = VisionTransformer().to(device)\n",
        "model.load_state_dict(torch.load(\"./best_model.pth\", map_location=device))  # Load your own trained weights\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "3pWcLARKt59J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compute accuracy of trained ViT on our test set:"
      ],
      "metadata": {
        "id": "uQvssHMVcVhv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in testloader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)  # Move to GPU if available\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = torch.max(outputs, 1)  # Get class with highest probability\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "accuracy = 100 * correct / total\n",
        "print(f'Test accuracy: {accuracy:.2f}%')"
      ],
      "metadata": {
        "id": "NkYr3mgpcAri"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see some examples of classification:"
      ],
      "metadata": {
        "id": "OQQJt22Vffj_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "testiter = iter(testloader)\n",
        "images, labels = next(testiter)\n",
        "input = images.to(device)\n",
        "output = model(input)\n",
        "_, predicted = torch.max(output, 1)\n",
        "\n",
        "i = random.randint(0, len(images))\n",
        "plt.axis('off')\n",
        "plt.imshow(to_plt(images[i]))\n",
        "print(\"True label: {}\".format(class_names[labels[i]]))\n",
        "print(\"Predicted: {}\".format(class_names[predicted[i]]))\n",
        "probs = torch.nn.functional.softmax(output[i])\n",
        "for cn, p in sorted(zip(class_names, probs), key=lambda x: -x[1]):\n",
        "  print(\"{}: {:.2f}%\".format(cn, p*100))"
      ],
      "metadata": {
        "id": "cKVIeND7y20y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Zero Shot Classification with CLIP"
      ],
      "metadata": {
        "id": "NP4eE6e2yMPI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CLIP** (Contrastive Language-Image Pre-Training) is a multi-modal model published by OpenAI in 2021. It is trained on **image-text pairs**.\n",
        "\n",
        "CLIP is composed of **two encoders** that embed images and captions (respectively) into the same vector space.\n",
        "\n",
        "<img src=\"https://cdn.sanity.io/images/vr8gru94/production/46b85e640919609e98a79a8142a1181e57d12799-2803x1335.png\" width=\"700\">\n",
        "\n",
        "The image encoder is a ViT (or a ResNet), the text encoder is a Transformer.\n",
        "\n",
        "Data are images with captions (not necessarily connected to \"class labels\"), easily found online.\n",
        "\n",
        "The two encoders are trained together using **contrastive learning**:\n",
        "\n",
        "<img src=\"https://cdn.sanity.io/images/vr8gru94/production/b7e270212d57ff9dd2da60f8fac00c6dddef9c3a-1165x671.png\" width=\"700\">\n",
        "\n",
        "*   Matching image-caption pairs must produce *similar* vectors\n",
        "*   Misplaced image-caption pairs (generated by swapping) must produce *dissimilar* vectors\n",
        "\n",
        "\n",
        "Similarity between vectors can be simply measured with **cosine similarity**:\n",
        "\n",
        "$$k(v, w) = \\frac{v^\\top w}{\\|v\\|_2\\|w\\|_2}$$\n",
        "\n",
        "that is just the cosine of the angle between the two vectors.\n",
        "\n",
        "This section is inspired by Eugenio Lomurno's CLIP notebook from the [ANNDL class](https://boracchi.faculty.polimi.it/teaching/AN2DL.htm).\n",
        "\n",
        "We will download a pre-trained CLIP model by [Hugging Face](https://github.com/huggingface) and use it to perform **zero-shot image classification** on CIFAR-10.\n",
        "\n",
        "Zero-shot means that we are using a model **pre-trained** on a task that is *not* classification to perform classification with **no extra training time**.\n",
        "\n",
        "We will just encode images and labels, then **measure to which label embedding the immage embedding is more similar**.\n",
        "\n",
        "<img src=\"https://cdn.sanity.io/images/vr8gru94/production/d9a6ebbc9a2f3334ec57a6b54d90155043c07595-1292x447.png\" width=\"700\">"
      ],
      "metadata": {
        "id": "V2o8pFczLj04"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "New and old imports (just in case):"
      ],
      "metadata": {
        "id": "3S55Bi1_g4WZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "26qhNFqUgzcj"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from transformers import CLIPProcessor, CLIPModel # https://github.com/huggingface/transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's set all random seeds for reproducibility:"
      ],
      "metadata": {
        "id": "hnmwPee9gzcl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "seed = 42\n",
        "\n",
        "def seed_everything(seed):\n",
        "  random.seed(seed)\n",
        "  np.random.seed(seed)\n",
        "  torch.manual_seed(seed)\n",
        "  if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "seed_everything(seed)"
      ],
      "metadata": {
        "id": "3Pmvj6ujgzcl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some visualization settings:"
      ],
      "metadata": {
        "id": "7WGUVM2tgzcl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set(font_scale=1.4)\n",
        "sns.set_style('white')\n",
        "plt.rc('font', size=14)\n",
        "%matplotlib inline\n",
        "\n",
        "# Function to display the image with pyplot (denormalize and transpose)\n",
        "def to_plt(img):\n",
        "  return np.transpose(img, (1, 2, 0)) * 0.5 + 0.5"
      ],
      "metadata": {
        "id": "2pnOzxc2gzcm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model and data preparation"
      ],
      "metadata": {
        "id": "8hkGSG1hluxI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We just download the test set in this case..."
      ],
      "metadata": {
        "id": "iP2yj28_lEEK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_transform = transforms.ToTensor()\n",
        "testset = datasets.CIFAR10(root='./data', train=False, download=True,\n",
        "                           transform=test_transform)\n",
        "\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=128, shuffle=False)\n",
        "\n",
        "class_names = ('airplane', 'automobile', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
      ],
      "metadata": {
        "id": "5niK6b0Y12xO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "...and the pre-trained model of course:"
      ],
      "metadata": {
        "id": "WQreFoYGmKOX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_id = \"openai/clip-vit-base-patch32\"\n",
        "\n",
        "# Try to use GPU\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Device: \", device)\n",
        "\n",
        "# CLIP will take care of data preprocessing\n",
        "processor = CLIPProcessor.from_pretrained(model_id)\n",
        "\n",
        "# Pre-trained model\n",
        "model = CLIPModel.from_pretrained(model_id)\n",
        "model.to(device)\n"
      ],
      "metadata": {
        "id": "RSWa9jl3yxoR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Zero shot classification"
      ],
      "metadata": {
        "id": "HA2tlQ2pt6Kp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The text encoder is expecting captions (full sentences), not one-word labels...so we generate **class prompts**:"
      ],
      "metadata": {
        "id": "acVkCIxemuXb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clip_labels = [f\"a photo of a {label}\" for label in class_names]\n",
        "clip_labels"
      ],
      "metadata": {
        "id": "vnr-p2tzy8MQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can embed the class prompts using CLIP's pre-trained **text encoder**:"
      ],
      "metadata": {
        "id": "eUO4k7MmnFUE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize label prompts\n",
        "label_tokens = processor(\n",
        "    text=clip_labels,\n",
        "    padding=True,\n",
        "    images=None,\n",
        "    return_tensors='pt'\n",
        ").to(device)\n",
        "\n",
        "# Transform into text embedding\n",
        "label_emb = model.get_text_features(**label_tokens)\n",
        "\n",
        "label_emb.shape"
      ],
      "metadata": {
        "id": "MpJfzFuRzi2R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Similarly, we can embed the images using CLIP's pre-trained **image encoder**.\n",
        "\n",
        "Let's pick a random image from our dataset:"
      ],
      "metadata": {
        "id": "RQj60ArDno9i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "testiter = iter(testloader)\n",
        "images, labels = next(testiter)\n",
        "\n",
        "i = 70\n",
        "#i = random.randint(0, len(images))\n",
        "print(i)\n",
        "img = images[i]\n",
        "plt.imshow(to_plt(img))\n",
        "plt.axis('off')\n",
        "print(\"True label: \", class_names[labels[i]])"
      ],
      "metadata": {
        "id": "5o8QNRngzotw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And embed it (after CLIP's pre-processing) into **the same 512-dimensional vector space** as the label prompts"
      ],
      "metadata": {
        "id": "eaD0fDM8n36U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image = processor(\n",
        "    text=None,\n",
        "    images=img,\n",
        "    return_tensors='pt',\n",
        "    do_rescale=False,\n",
        ")['pixel_values'].to(device)\n",
        "\n",
        "img_emb = model.get_image_features(image)\n",
        "img_emb.shape"
      ],
      "metadata": {
        "id": "m_gRy96S0yXh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's detach the two embeddings, move them to cpu, and convert into numpy arrays (you can skip this last part if you want to continue using pytorch)"
      ],
      "metadata": {
        "id": "-ZJCcWbfowdQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "label_emb = label_emb.detach().cpu().numpy()\n",
        "img_emb = img_emb.detach().cpu().numpy()"
      ],
      "metadata": {
        "id": "mt-BGPyV1U0g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TODO:** compute cosine similarity scores between the image embedding and each of the label embeddings. The label with the largest similarity: that is your class prediction"
      ],
      "metadata": {
        "id": "ye-dsB_9qto5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scores = ... # YOUR CODE HERE\n",
        "\n",
        "print(scores)\n",
        "print(class_names[np.argmax(scores)])"
      ],
      "metadata": {
        "id": "lEAiDluT2-Rg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualization\n",
        "\n",
        "It is not easy to visualize 512-dimensional vectors...however we can display the (scalar) **angles** between the image embedding and each label embedding on the unit circle:"
      ],
      "metadata": {
        "id": "WZEUfgK8MVt1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "angles = np.arccos(scores)[0]\n",
        "print(np.rad2deg(angles))\n",
        "\n",
        "colors = plt.cm.tab10(np.linspace(0, 1, len(angles)))  # Use a colormap for distinct colors\n",
        "\n",
        "# Compute x, y coordinates on the unit circle\n",
        "x = np.cos(angles)\n",
        "y = np.sin(angles)\n",
        "\n",
        "# Create figure and axis\n",
        "fig, ax = plt.subplots(figsize=(6, 6))\n",
        "ax.set_xlim(0, 1.2)\n",
        "ax.set_ylim(-0.1, 1.2)\n",
        "ax.set_aspect('equal')\n",
        "ax.axhline(0, color='gray', linewidth=0.5)\n",
        "ax.axvline(0, color='gray', linewidth=0.5)\n",
        "\n",
        "# Draw the unit circle (only first quadrant)\n",
        "theta = np.linspace(0, np.pi / 2, 100)\n",
        "ax.plot(np.cos(theta), np.sin(theta), color='lightblue', linewidth=2)\n",
        "\n",
        "# Plot points on the circle with different colors\n",
        "for i, (xi, yi, color) in enumerate(zip(x, y, colors)):\n",
        "    ax.scatter(xi, yi, color=color, zorder=3, label=class_names[i])\n",
        "ax.scatter(1, 0, marker='x', label='Image')\n",
        "\n",
        "\n",
        "# Add legend\n",
        "ax.legend(loc=\"upper right\", fontsize=9, title_fontsize=10)\n",
        "\n",
        "# Labels\n",
        "ax.set_xticks([])\n",
        "ax.set_yticks([])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "euV7YiOU5iPh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Another thing we can do is using **TSNE** to obtain a visualization of the high-dimensional embeddings in 2D or 3D space. We will use *cosine distance* (wich is just one minus cosine similarity) as metric to reflect our classification criterion."
      ],
      "metadata": {
        "id": "5hR-TrxXMslw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.manifold import TSNE\n",
        "from sklearn.metrics.pairwise import cosine_distances\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(6, 6))\n",
        "colors = plt.cm.tab10(np.linspace(0, 1, len(angles)))  # Use a colormap for distinct colors\n",
        "\n",
        "# Stack into a single NumPy array\n",
        "data = np.vstack((label_emb, img_emb))\n",
        "\n",
        "cosine_dist = cosine_distances(data)\n",
        "tsne = TSNE(n_components=2, metric=\"precomputed\", init=\"random\", perplexity=6, random_state=seed)\n",
        "embedded = tsne.fit_transform(cosine_dist)\n",
        "\n",
        "x = embedded[:-1, 0]\n",
        "y = embedded[:-1, 1]\n",
        "\n",
        "for i, (xi, yi, color) in enumerate(zip(x, y, colors)):\n",
        "    ax.scatter(xi, yi, color=color, zorder=3, label=class_names[i])\n",
        "ax.scatter(embedded[-1, 0], embedded[-1, 1], marker='x', label='Image')\n",
        "\n",
        "ax.legend(loc=\"lower left\", fontsize=9, title_fontsize=10)\n",
        "\n",
        "ax.set_xticks([])\n",
        "ax.set_yticks([])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "oGAEpWcCxQMc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TODO:** let's turn these code snippets into a zero-shot image classifier:"
      ],
      "metadata": {
        "id": "fJDY1IrmrTnV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def zero_shot_classify(images, candidate_labels):\n",
        "  \"\"\"\n",
        "    Inputs:\n",
        "      images - A batch of images of shape (Batchsize, Channels, Width, Height)\n",
        "      candidate_labels - A list of class names (strings)\n",
        "    Returns: class predictions as indexes between 0 and len(candidate_labels),\n",
        "             one for each image,\n",
        "             as an array of shape (Batchsize,)\n",
        "  \"\"\"\n",
        "  return ... # YOUR CODE HERE"
      ],
      "metadata": {
        "id": "07TZz-vL5Cop"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's try it on our dataset:"
      ],
      "metadata": {
        "id": "onpIdXCktrYf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "testiter = iter(testloader)\n",
        "images, labels = next(testiter)\n",
        "predicted = zero_shot_classify(images, class_names)"
      ],
      "metadata": {
        "id": "gIfnMI6u5zqg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see the result on some random images first:"
      ],
      "metadata": {
        "id": "HmZVo3smuCVR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "i = 11\n",
        "#i = random.randint(0, len(images))\n",
        "img = images[i]\n",
        "\n",
        "plt.imshow(to_plt(img))\n",
        "print(\"True label: \", class_names[labels[i]])\n",
        "print(\"Predicted: \", class_names[predicted[i]])"
      ],
      "metadata": {
        "id": "LT3QOXNEQOpg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, let's measure the accuracy:"
      ],
      "metadata": {
        "id": "fyhPcOutuG3Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "correct = 0\n",
        "total = len(images)\n",
        "\n",
        "with torch.no_grad():\n",
        "    for img, lab, pred in zip(images, labels, predicted):\n",
        "        correct += int(pred == lab)\n",
        "accuracy = 100 * correct / total\n",
        "print(f'Test accuracy: {accuracy:.2f}%')"
      ],
      "metadata": {
        "id": "u1NTLcNY6ERp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vFgCHRYFHeRT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}